{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU openai pinecone-client datasets\n",
    "!pip install --upgrade tiktoken\n",
    "!pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "\n",
    "class SQLiteHandler:\n",
    "    def __init__(self, db_name):\n",
    "        self.db_name = db_name\n",
    "        self.db_table_brain = 'brain'\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "        self.create_connection_and_table()\n",
    "\n",
    "    def create_connection_and_table(self):\n",
    "        if not os.path.exists(self.db_name):\n",
    "            self.conn = sqlite3.connect(self.db_name)\n",
    "            self.create_table()\n",
    "        else:\n",
    "            self.conn = sqlite3.connect(self.db_name)\n",
    "            if not self.check_table_exists(self.db_table_brain):\n",
    "                self.create_table()\n",
    "        self.cursor = self.conn.cursor()\n",
    "        #self.conn.set_trace_callback(print)\n",
    "\n",
    "    def check_table_exists(self, table_name):\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}';\")\n",
    "        return bool(self.cursor.fetchall())\n",
    "\n",
    "    def create_table(self):\n",
    "        query = f\"\"\"CREATE TABLE {self.db_table_brain} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        message TEXT,\n",
    "                        entities TEXT\n",
    "                    );\"\"\"\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(query)\n",
    "        self.conn.commit()\n",
    "\n",
    "    def insert_data(self, table_name, data):\n",
    "        data = {k: json.dumps(v) if isinstance(v, list) else v for k, v in data.items()}\n",
    "        columns = ', '.join(data.keys())\n",
    "        values = ', '.join(['?'] * len(data))\n",
    "        query = f\"INSERT INTO {table_name} ({columns}) VALUES ({values})\"\n",
    "        self.cursor.execute(query, list(data.values()))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def close_connection(self):\n",
    "        self.conn.close()\n",
    "\n",
    "    def print_match(self, search_for):\n",
    "        self.cursor.execute(f\"SELECT body FROM {self.db_table_brain}\")\n",
    "        rows = self.cursor.fetchall()\n",
    "        for row in rows:\n",
    "            print(f\"{row}\")\n",
    "\n",
    "    def get_id(self):\n",
    "        self.cursor.execute(\"SELECT last_insert_rowid()\")\n",
    "        return self.cursor.fetchone()[0]\n",
    "\n",
    "    def print_select(self, select_txt):\n",
    "        self.cursor.execute(f\"{select_txt}\")\n",
    "        rows = self.cursor.fetchall()\n",
    "        for row in rows:\n",
    "            print(f\"{row}\")\n",
    "\n",
    "    def print_data(self):\n",
    "        self.cursor.execute(f\"SELECT body FROM {self.db_table_brain}\")\n",
    "        rows = self.cursor.fetchall()\n",
    "        for row in rows:\n",
    "            print(f\"{row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize local database (LTSM) to store notes\n",
    "DATABASE = os.getcwd() + \"/brain02.db\"\n",
    "print(DATABASE)\n",
    "handler = SQLiteHandler(DATABASE)\n",
    "# do you want to load and index data?\n",
    "LOAD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai, tiktoken\n",
    "import pinecone\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "picone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "picone_env = \"us-west1-gcp-free\"\n",
    "\n",
    "MODEL3 = \"gpt-3.5-turbo\"\n",
    "MODEL4 = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scratch some text data (wiki scratacher) part 1 of 2\n",
    "import wikipediaapi\n",
    "\n",
    "def get_page_text(page_title, language):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(language)\n",
    "\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    if page.exists():\n",
    "        return page.text\n",
    "    else:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare some function from openai to use them later\n",
    "\n",
    "# chat complition\n",
    "def chat_completion(model, system_text, user_text):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_text},\n",
    "            {\"role\": \"user\", \"content\": user_text},\n",
    "        ],\n",
    "    )\n",
    "    message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return message\n",
    "# provide number of tokens in string\n",
    "def num_tokens_from_string(string: str, model: str = \"gpt-3.5-turbo-0301\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# provide number of tokens in real chat conversation of openai format\n",
    "def num_tokens_from_messages(messages, model=MODEL3):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how it works for a simple string and wiki page for Warszawa\n",
    "#print(num_tokens_from_string(\"testing openai API\", MODEL3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embeding\n",
    "def get_embeddings(input):\n",
    "    embed_model = \"text-embedding-ada-002\"\n",
    "    res = openai.Embedding.create(\n",
    "        input=input,\n",
    "        engine=embed_model\n",
    "    )\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare picone connection\n",
    "pinecone.init(api_key=picone_api_key, environment=picone_env)\n",
    "index_name=\"qa\"\n",
    "index_lst = pinecone.describe_index(index_name) #GRPCIndex(index_name)\n",
    "print(index_lst)\n",
    "index = pinecone.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to split the text into smaller chunks\n",
    "# https://python.langchain.com/en/latest/modules/indexes/text_splitters/getting_started.html\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = num_tokens_from_string,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD:\n",
    "    #scratch some text data (get data) part 2 of 2\n",
    "    language = \"pl\"  \n",
    "    for page_titel in [\"Mikołaj_Kopernik\"]: # Kraków Warszawa Mikołaj_Kopernik\n",
    "        page_txt = get_page_text(page_titel, language)\n",
    "        if page_txt:\n",
    "            print(f\"Processing {page_titel} page...\")\n",
    "            texts = text_splitter.create_documents([page_txt])\n",
    "            # add to local brain and embedings to picone\n",
    "            for chunk in texts:\n",
    "                # print(chunk.page_content)\n",
    "                handler.insert_data(handler.db_table_brain, {\"message\" : chunk.page_content})\n",
    "                handler.conn.commit()\n",
    "                # extract embeddings to a list\n",
    "                embeds = [record['embedding'] for record in (get_embeddings([chunk.page_content]))['data']][0]\n",
    "                chunk_id = str(handler.get_id())\n",
    "                to_upsert = (chunk_id, embeds, {})\n",
    "                # print (chunk_id)\n",
    "                \n",
    "                upsert_response = index.upsert(\n",
    "                    vectors=[to_upsert],\n",
    "                #     namespace=\"example-namespace\"\n",
    "                )\n",
    "        else:\n",
    "            print(f\"Page {page_titel} does not exist.\")\n",
    "handler.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Pierwszymi osadami powstałymi w obecnych granicach administracyjnych Warszawy były\n",
      "id: 14, score: 0.869876146\n",
      "id: 16, score: 0.86975044\n",
      "id: 23, score: 0.864600122\n",
      "text: Jakie osady powstały w granicach Warszawy?\n",
      "id: 16, score: 0.878372133\n",
      "id: 14, score: 0.875454962\n",
      "id: 23, score: 0.870916069\n",
      "text: XXXX\n",
      "id: 96, score: 0.746123075\n",
      "id: 19, score: 0.745900571\n",
      "id: 116, score: 0.745182812\n"
     ]
    }
   ],
   "source": [
    "# test query\n",
    "for query_txt in [\n",
    "    \"Pierwszymi osadami powstałymi w obecnych granicach administracyjnych Warszawy były\",\n",
    "    \"Jakie osady powstały w granicach Warszawy?\",\n",
    "    \"XXXX\"    \n",
    "]:\n",
    "    embeds = [record['embedding'] for record in (get_embeddings([query_txt]))['data']][0]\n",
    "    query_response = index.query(\n",
    "        top_k=3,\n",
    "        include_values=True,\n",
    "        include_metadata=False,\n",
    "        vector=embeds\n",
    "    )[\"matches\"]\n",
    "    print(f\"text: {query_txt}\")\n",
    "    for item in query_response:\n",
    "        print(f\"id: {item.id}, score: {item.score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
